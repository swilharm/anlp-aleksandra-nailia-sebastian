{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"So2KDTNqLXdY"},"outputs":[],"source":["%%capture\n","!pip install wget\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6UpdQta0qOz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647768886850,"user_tz":-60,"elapsed":2193,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"ab685283-a668-47ce-8147-081e9d28f5bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import sys\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/MyDrive/project/libs')\n","model_path = '/content/drive/MyDrive/project/models'\n","training_path = '/content/drive/MyDrive/project/dataset/training'\n","test_path = '/content/drive/MyDrive/project/dataset/test'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrOoFN43Lfjr"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel\n","from torch import nn\n","from torch.optim import Adam\n","from tqdm import tqdm\n","\n","from IPython.display import Image, display\n","import PIL.Image\n","import io\n","from processing_image import Preprocess\n","from visualizing_image import SingleImageViz\n","from modeling_frcnn import GeneralizedRCNN\n","from utils import Config\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mu3ACfWEfrPm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647768891053,"user_tz":-60,"elapsed":1753,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"6b8b1b95-1074-4b85-acda-e25150ff6c61"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 3)"]},"metadata":{},"execution_count":4}],"source":["train_labels = pd.read_csv(training_path+'/training.csv', sep='\\t', header=0, names=['file_name', 'misogynous',\t'shaming',\t'stereotype',\t'objectification',\t'violence', 'text'])\n","train_labels = train_labels[['file_name', 'misogynous', 'text']]\n","train_labels.to_csv(training_path+'/labels.csv', index=None, sep='\\t')\n","train_labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBMWz-GLVtRo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647768892943,"user_tz":-60,"elapsed":1892,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"c504a08d-9a89-461d-fb57-def990121b41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 3)"]},"metadata":{},"execution_count":5}],"source":["test_data = pd.read_csv(test_path+'/test.csv', sep='\\t', header=0, names=['file_name', 'text'])\n","test_labels = pd.read_csv(test_path+'/test_labels.txt', sep='\\t', header=None, names=['file_name', \"misogynous\",\t\"shaming\",\t\"stereotype\",\t\"objectification\",\t\"violence\"])\n","test_labels = pd.merge(test_labels, test_data, on='file_name')[['file_name', 'misogynous', 'text']]\n","test_labels.to_csv(test_path+'/labels.csv', index=None, sep='\\t')\n","test_labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgs3HZpkpFEV","outputId":"260e8dc9-b49b-4ef1-c2e7-ebe974ceb60b","executionInfo":{"status":"ok","timestamp":1647768892944,"user_tz":-60,"elapsed":7,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 3)"]},"metadata":{},"execution_count":6}],"source":["train_labels = pd.read_csv('/content/drive/MyDrive/project/dataset/training_labels.csv', sep='\\t')\n","train_labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfnlFn-igoUl","executionInfo":{"status":"ok","timestamp":1647768892945,"user_tz":-60,"elapsed":5,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"97d8e6dc-25ba-4233-e694-26ea8248b705"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 3)"]},"metadata":{},"execution_count":7}],"source":["test_labels = pd.read_csv('/content/drive/MyDrive/project/dataset/test_labels.csv', sep='\\t')\n","test_labels.shape"]},{"cell_type":"code","source":["%%capture\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n","frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg).to(device)\n","image_preprocess = Preprocess(frcnn_cfg)"],"metadata":{"id":"L03ofzRUtcja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","train_preprocessed = pd.read_pickle(training_path+'/preprocessed_cpu.pickle')\n","processed_files = set(train_preprocessed['file_name'])\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","train_preprocessed = pd.DataFrame(columns=['file_name', 'misogynous', 'text', 'visual_embeds', 'input_ids', 'token_type_ids', 'attention_mask'])\n","for index, row in tqdm(train_labels.iterrows(), total=train_labels.shape[0]):\n","  if row[file_name] in processed_files:\n","    continue\n","  images, sizes, scales_yx = image_preprocess(training_path+'/'+row['file_name'])\n","  output_dict = frcnn(\n","      images.to(device),\n","      sizes.to(device),\n","      scales_yx=scales_yx.to(device),\n","      padding=\"max_detections\",\n","      max_detections=512,\n","      return_tensors=\"pt\",\n","  )\n","  image_preprocessed = output_dict.get(\"roi_features\")\n","  text_preprocessed = tokenizer(row['text'], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","  train_preprocessed = train_preprocessed.append(\n","      {\n","       'file_name': row['file_name'], \n","       'misogynous': row['misogynous'],\n","       'text': row['text'],\n","       'visual_embeds': image_preprocessed,\n","       'input_ids': text_preprocessed['input_ids'],\n","       'token_type_ids': text_preprocessed['token_type_ids'],\n","       'attention_mask': text_preprocessed['attention_mask']\n","      },\n","      ignore_index=True\n","  )\n","  if index%500 == 0:\n","      train_preprocessed.to_pickle('/content/drive/MyDrive/project/dataset/training_preprocessed.pickle')\n","      print(f\"Process saved: {index}\")\n","train_preprocessed.to_pickle('/content/drive/MyDrive/project/dataset/training_preprocessed.pickle')\n","'''\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWJezqg2uNdD","executionInfo":{"status":"ok","timestamp":1647768909662,"user_tz":-60,"elapsed":18,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"4ef213d9-6ef9-4dee-c1cb-9981be723e7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["'''\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","test_preprocessed = pd.DataFrame(columns=['file_name', 'misogynous', 'text', 'visual_embeds', 'input_ids', 'token_type_ids', 'attention_mask'])\n","for index, row in tqdm(test_labels.iterrows(), total=test_labels.shape[0]):\n","  images, sizes, scales_yx = image_preprocess(test_path+'/'+row['file_name'])\n","  output_dict = frcnn(\n","      images.to(device),\n","      sizes.to(device),\n","      scales_yx=scales_yx.to(device),\n","      padding=\"max_detections\",\n","      max_detections=512,\n","      return_tensors=\"pt\",\n","  )\n","  image_preprocessed = output_dict.get(\"roi_features\")\n","  text_preprocessed = tokenizer(row['text'], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","  test_preprocessed = test_preprocessed.append(\n","      {\n","       'file_name': row['file_name'], \n","       'misogynous': row['misogynous'],\n","       'text': row['text'],\n","       'visual_embeds': image_preprocessed,\n","       'input_ids': text_preprocessed['input_ids'],\n","       'token_type_ids': text_preprocessed['token_type_ids'],\n","       'attention_mask': text_preprocessed['attention_mask']\n","      },\n","      ignore_index=True\n","  )\n","#test_preprocessed.to_pickle('/content/drive/MyDrive/project/dataset/test_preprocessed.pickle')\n","'''\n","print()"],"metadata":{"id":"JQGg6MAD7DBY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647768909663,"user_tz":-60,"elapsed":17,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"0e9ca827-776e-45fc-d8e1-6976db68f5e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["#train_preprocessed = pd.read_pickle(training_path+'/preprocessed_cpu.pickle')\n","#test_preprocessed = pd.read_pickle('/content/drive/MyDrive/project/dataset/test_preprocessed.pickle')"],"metadata":{"id":"AaYv5AIvAUlL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8OphndbNbXe"},"outputs":[],"source":["labels = {'0':0,\n","          '1':1\n","          }\n","\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","        #self.images = [image for image in df['visual_embeds']]\n","        self.images = [image for image in df['file_name']]\n","        self.labels = [labels[str(label)] for label in df['misogynous']]\n","        #self.texts = [{'input_ids': text['input_ids'], 'token_type_ids': text['token_type_ids'], 'attention_mask': text['attention_mask']} for _,text in df[['input_ids', 'token_type_ids', 'attention_mask']].iterrows()]\n","        self.texts = [text for text in df['text']]\n","        \n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of text inputs\n","        return self.texts[idx]\n","\n","    def get_batch_images(self, idx):\n","        # Fetch a batch of image inputs\n","        return self.images[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_images = self.get_batch_images(idx)\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_images, batch_texts, batch_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2on7FCMGNi5S"},"outputs":[],"source":["class VisualBertClassifier(nn.Module):\n","\n","    def __init__(self, dropout=0.5):\n","        super(VisualBertClassifier, self).__init__()\n","\n","        self.model = AutoModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, 2)\n","        #self.relu = nn.ReLU()\n","\n","    def forward(self, inputs):\n","      pooled_output = self.model(**inputs)['pooler_output']\n","      dropout_output = self.dropout(pooled_output)\n","      linear_output = self.linear(dropout_output)\n","      #final_layer = self.relu(linear_output)\n","      return linear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNGdiSmdNsQi"},"outputs":[],"source":["def train(model, train_data, val_data, learning_rate, epochs):\n","\n","    train, val = Dataset(train_data), Dataset(val_data)\n","\n","    train_dataloader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True)\n","    val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    criterion = nn.BCELoss()\n","    optimizer = Adam(model.parameters(), lr= learning_rate)\n","\n","    if use_cuda:\n","\n","            model = model.cuda()\n","            criterion = criterion.cuda()\n","\n","    for epoch_num in range(epochs):\n","\n","            total_acc_train = 0\n","            total_loss_train = 0\n","\n","            for train_image, train_text, train_label in tqdm(train_dataloader):\n","\n","                train_label = train_label.to(device)\n","                text_preprocessed = tokenizer(train_text[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","                images, sizes, scales_yx = image_preprocess(training_path+'/'+train_image[0])\n","                output_dict = frcnn(\n","                    images.to(device),\n","                    sizes.to(device),\n","                    scales_yx=scales_yx.to(device),\n","                    padding=\"max_detections\",\n","                    max_detections=512,\n","                    return_tensors=\"pt\",\n","                )\n","                image_preprocessed = output_dict.get(\"roi_features\")\n","                train_inputs = (\n","                    {\n","                      \"input_ids\": text_preprocessed['input_ids'].squeeze(1).to(device),\n","                      \"token_type_ids\": text_preprocessed['token_type_ids'].squeeze(1).to(device),\n","                      \"attention_mask\": text_preprocessed['attention_mask'].squeeze(1).to(device),\n","                      \"visual_embeds\": image_preprocessed.squeeze(1).to(device),\n","                      \"visual_token_type_ids\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                      \"visual_attention_mask\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                      \"output_attentions\": False,\n","                      \"output_hidden_states\": False\n","                    }\n","                )\n","                output = model(train_inputs)\n","                \n","                batch_loss = criterion(output, train_label)\n","                total_loss_train += batch_loss.item()\n","                \n","                acc = (output.argmax(dim=1) == train_label).sum().item()\n","                total_acc_train += acc\n","                \n","                model.zero_grad()\n","                batch_loss.backward()\n","                optimizer.step()\n","            \n","            total_acc_val = 0\n","            total_loss_val = 0\n","\n","            with torch.no_grad():\n","\n","                for val_image, val_text, val_label in val_dataloader:\n","\n","                    val_label = val_label.to(device)\n","                    text_preprocessed = tokenizer(val_text[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","                    images, sizes, scales_yx = image_preprocess(training_path+'/'+val_image[0])\n","                    output_dict = frcnn(\n","                        images.to(device),\n","                        sizes.to(device),\n","                        scales_yx=scales_yx.to(device),\n","                        padding=\"max_detections\",\n","                        max_detections=512,\n","                        return_tensors=\"pt\",\n","                    )\n","                    image_preprocessed = output_dict.get(\"roi_features\")\n","                    val_inputs = (\n","                        {\n","                          \"input_ids\": text_preprocessed['input_ids'].squeeze(1).to(device),\n","                          \"token_type_ids\": text_preprocessed['token_type_ids'].squeeze(1).to(device),\n","                          \"attention_mask\": text_preprocessed['attention_mask'].squeeze(1).to(device),\n","                          \"visual_embeds\": image_preprocessed.squeeze(1).to(device),\n","                          \"visual_token_type_ids\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                          \"visual_attention_mask\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                          \"output_attentions\": False,\n","                          \"output_hidden_states\": False\n","                        }\n","                    )\n","\n","                    output = model(val_inputs)\n","\n","                    batch_loss = criterion(output, val_label)\n","                    total_loss_val += batch_loss.item()\n","                    \n","                    acc = (output.argmax(dim=1) == val_label).sum().item()\n","                    total_acc_val += acc\n","            \n","            print(\n","                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n","            torch.save(model.state_dict(), f\"{model_path}/{epoch_num + 1}.bin\")\n","                  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7ITbusbNwqG"},"outputs":[],"source":["def evaluate(model, test_data):\n","\n","    test = Dataset(test_data)\n","\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","\n","        model = model.cuda()\n","\n","    total_acc_test = 0\n","    with torch.no_grad():\n","\n","        for test_image, test_text, test_label in tqdm(test_dataloader):\n","\n","              test_label = test_label.to(device)\n","              text_preprocessed = tokenizer(test_text[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","\n","              images, sizes, scales_yx = image_preprocess(test_path+'/'+test_image[0])\n","              output_dict = frcnn(\n","                  images.to(device),\n","                  sizes.to(device),\n","                  scales_yx=scales_yx.to(device),\n","                  padding=\"max_detections\",\n","                  max_detections=512,\n","                  return_tensors=\"pt\",\n","              )\n","              image_preprocessed = output_dict.get(\"roi_features\")\n","              test_inputs = (\n","                  {\n","                    \"input_ids\": text_preprocessed['input_ids'].squeeze(1).to(device),\n","                    \"token_type_ids\": text_preprocessed['token_type_ids'].squeeze(1).to(device),\n","                    \"attention_mask\": text_preprocessed['attention_mask'].squeeze(1).to(device),\n","                    \"visual_embeds\": image_preprocessed.squeeze(1).to(device),\n","                    \"visual_token_type_ids\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                    \"visual_attention_mask\": torch.ones(image_preprocessed.squeeze(1).shape[:-1], dtype=torch.long).to(device),\n","                    \"output_attentions\": False,\n","                    \"output_hidden_states\": False\n","                  }\n","              )\n","\n","              output = model(test_inputs)\n","\n","              acc = (output.argmax(dim=1) == test_label).sum().item()\n","              total_acc_test += acc\n","    \n","    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUB728yRNz1N","outputId":"f57c5921-2d75-433e-a205-77762d1eebc0","executionInfo":{"status":"ok","timestamp":1647768910829,"user_tz":-60,"elapsed":5,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["900 100\n"]}],"source":["np.random.seed(112)\n","df_train, df_val = np.split(train_labels.head(1000).sample(frac=1, random_state=42), [int(.9*len(train_labels.head(1000)))])\n","\n","print(len(df_train),len(df_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Me64ACR-N3ED","outputId":"7cbd6b4c-3c9f-4cb8-9d2a-7fde2303e525","executionInfo":{"status":"ok","timestamp":1647772788001,"user_tz":-60,"elapsed":3877176,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","  0%|          | 0/900 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","100%|██████████| 900/900 [19:53<00:00,  1.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 1 | Train Loss:  0.694 | Train Accuracy:  0.488 | Val Loss:  0.684 | Val Accuracy:  0.550\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 900/900 [19:46<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 2 | Train Loss:  0.696 | Train Accuracy:  0.477 | Val Loss:  0.692 | Val Accuracy:  0.570\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 900/900 [19:45<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 3 | Train Loss:  0.693 | Train Accuracy:  0.489 | Val Loss:  0.708 | Val Accuracy:  0.550\n"]}],"source":["EPOCHS = 3\n","model = VisualBertClassifier()\n","LR = 1e-6\n","              \n","train(model, df_train, df_val, LR, EPOCHS)"]},{"cell_type":"code","source":["model = VisualBertClassifier()\n","model.load_state_dict(torch.load(f\"{model_path}/3.bin\", map_location=torch.device('cpu')))\n","model.eval()\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["5f528dfbd70e41b6a35d2d3e715278a9","4b559e6b30204d65b98922afcb4ab310","2a9383c5628f431290eb38ee55a61fcb","bbb6510ae5794655b23f90ec6d43ab11","1d56ae4e4c68464f868a3aa923887d67","222644c1dbae49d4ba4c792ca885332e","cacc0b31b63848ef8f6c19bfde5607b4","3c3ac7c2424f44729b9c149429d3d299","a668eecf4fe44af5bc65ec4a2e7e59bc","ef895748c8bd4222ae7575aec07dd64d","930219e019024c4c86fe73b144b7979a","febbb5cb365a46048b8961dc57cac4ed","dc897c63ea1444d7993c251f849e0f4e","c22c03ad2887444a88bb8ba43d3f4d27","7af1d8210f5b4107b9bfeab375f66819","06661a7bc1d84f3b93175771944c0112","4167ad0c5dd44200aa108d7d40bc1356","67a53c3428b24fcc8daf81088d5ac82f","53b1c26cfe804a5f88328ea5cbc707dd","a2d8f536823d4102923a02e6799b37f7","e2edb71cf1444ff2879de501f6128ca4","610fcb34ff5a494c9c678ca7d47de4e5"]},"id":"7NR_rCe6P6bo","executionInfo":{"status":"ok","timestamp":1647765772323,"user_tz":-60,"elapsed":29005,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"377f4beb-c8e5-49fc-fe18-0c81abf11694"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/631 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f528dfbd70e41b6a35d2d3e715278a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/428M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"febbb5cb365a46048b8961dc57cac4ed"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["evaluate(model, test_labels.head(100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEwPv9vNYskk","executionInfo":{"status":"ok","timestamp":1647772958642,"user_tz":-60,"elapsed":170655,"user":{"displayName":"Sebastian Wilharm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09202640292167734258"}},"outputId":"ba3bb26e-658f-47ee-dab2-4f79473f533c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [02:50<00:00,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy:  0.530\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"065cmA-yPwtN"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"VisualBert_anlp.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5f528dfbd70e41b6a35d2d3e715278a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b559e6b30204d65b98922afcb4ab310","IPY_MODEL_2a9383c5628f431290eb38ee55a61fcb","IPY_MODEL_bbb6510ae5794655b23f90ec6d43ab11"],"layout":"IPY_MODEL_1d56ae4e4c68464f868a3aa923887d67"}},"4b559e6b30204d65b98922afcb4ab310":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222644c1dbae49d4ba4c792ca885332e","placeholder":"​","style":"IPY_MODEL_cacc0b31b63848ef8f6c19bfde5607b4","value":"Downloading: 100%"}},"2a9383c5628f431290eb38ee55a61fcb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c3ac7c2424f44729b9c149429d3d299","max":631,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a668eecf4fe44af5bc65ec4a2e7e59bc","value":631}},"bbb6510ae5794655b23f90ec6d43ab11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef895748c8bd4222ae7575aec07dd64d","placeholder":"​","style":"IPY_MODEL_930219e019024c4c86fe73b144b7979a","value":" 631/631 [00:00&lt;00:00, 14.6kB/s]"}},"1d56ae4e4c68464f868a3aa923887d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"222644c1dbae49d4ba4c792ca885332e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cacc0b31b63848ef8f6c19bfde5607b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c3ac7c2424f44729b9c149429d3d299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a668eecf4fe44af5bc65ec4a2e7e59bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef895748c8bd4222ae7575aec07dd64d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"930219e019024c4c86fe73b144b7979a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"febbb5cb365a46048b8961dc57cac4ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc897c63ea1444d7993c251f849e0f4e","IPY_MODEL_c22c03ad2887444a88bb8ba43d3f4d27","IPY_MODEL_7af1d8210f5b4107b9bfeab375f66819"],"layout":"IPY_MODEL_06661a7bc1d84f3b93175771944c0112"}},"dc897c63ea1444d7993c251f849e0f4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4167ad0c5dd44200aa108d7d40bc1356","placeholder":"​","style":"IPY_MODEL_67a53c3428b24fcc8daf81088d5ac82f","value":"Downloading: 100%"}},"c22c03ad2887444a88bb8ba43d3f4d27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53b1c26cfe804a5f88328ea5cbc707dd","max":448356189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2d8f536823d4102923a02e6799b37f7","value":448356189}},"7af1d8210f5b4107b9bfeab375f66819":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2edb71cf1444ff2879de501f6128ca4","placeholder":"​","style":"IPY_MODEL_610fcb34ff5a494c9c678ca7d47de4e5","value":" 428M/428M [00:16&lt;00:00, 33.7MB/s]"}},"06661a7bc1d84f3b93175771944c0112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167ad0c5dd44200aa108d7d40bc1356":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67a53c3428b24fcc8daf81088d5ac82f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53b1c26cfe804a5f88328ea5cbc707dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2d8f536823d4102923a02e6799b37f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2edb71cf1444ff2879de501f6128ca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"610fcb34ff5a494c9c678ca7d47de4e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}